From 1dbabb39b93d9d54f7c31a965c93c8078d460cc0 Mon Sep 17 00:00:00 2001
From: luca abeni <luca.abeni@santannapisa.it>
Date: Wed, 24 Jan 2018 12:50:07 +0100
Subject: [PATCH 07/10] Other assorted fixups

Again, to be split / merged in previous patches
---
 kernel/sched/deadline.c | 20 ++++++++++++--------
 kernel/sched/rt.c       | 10 ++++++++--
 2 files changed, 20 insertions(+), 10 deletions(-)

diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 24a6f22809c2..0d7f915c3ccb 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -196,22 +196,26 @@ int dl_check_tg(unsigned long total)
 int dl_init_tg(struct sched_dl_entity *dl_se, u64 rt_runtime, u64 rt_period)
 {
 	struct rq *rq = container_of(dl_rq_of_se(dl_se), struct rq, dl);
+	int is_active;
+
+	is_active = !dl_se->dl_throttled && (rt_rq_of_dl_entity(dl_se)->rt_nr_running > 0);
 
 	raw_spin_lock_irq(&rq->lock);
 	dl_se->dl_runtime  = rt_runtime;
 	dl_se->dl_period   = rt_period;
 	dl_se->dl_deadline = dl_se->dl_period;
+	if (is_active) {
+		sub_running_bw(dl_se, dl_rq_of_se(dl_se));
+	} else if (dl_se->dl_non_contending) {
+		sub_running_bw(dl_se, dl_rq_of_se(dl_se));
+		dl_se->dl_non_contending = 0;
+		hrtimer_try_to_cancel(&dl_se->inactive_timer);
+	}
 	sub_rq_bw(dl_se, dl_rq_of_se(dl_se));
 	dl_se->dl_bw = to_ratio(dl_se->dl_period, dl_se->dl_runtime);
 	add_rq_bw(dl_se, dl_rq_of_se(dl_se));
-
-	/* ??? FIX THIS CRAP! ??? */
-	if (!((s64)(rt_period - rt_runtime) >= 0) ||
-	    !(rt_runtime >= (2 << (DL_SCALE - 1)))) {
-		raw_spin_unlock_irq(&rq->lock);
-
-		return 0;
-	}
+	if (is_active)
+		add_running_bw(dl_se, dl_rq_of_se(dl_se));
 
 	raw_spin_unlock_irq(&rq->lock);
 
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index 05d6d4d10e21..4b3403cb3a5d 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -2017,6 +2017,13 @@ static int __rt_schedulable(struct task_group *tg, u64 period, u64 runtime)
 		.rt_runtime = runtime,
 	};
 
+	if (!((s64)(period - runtime) >= 0) ||
+	    (runtime && !(runtime >= (2 << (DL_SCALE - 1))))) {
+
+		return 1;
+	}
+
+
 	rcu_read_lock();
 	ret = walk_tg_tree(tg_rt_schedulable, tg_nop, &data);
 	rcu_read_unlock();
@@ -2054,8 +2061,7 @@ static int tg_set_rt_bandwidth(struct task_group *tg,
 		goto unlock_bandwidth;
 
 	for_each_possible_cpu(i) {
-printk_deferred("Initing TG[%d]: %llu %llu\n", i, rt_runtime, rt_period);
-		if (dl_init_tg(tg->dl_se[i], rt_runtime, rt_period) == 0) continue;
+		dl_init_tg(tg->dl_se[i], rt_runtime, rt_period);
 	}
 unlock_bandwidth:
 	raw_spin_unlock_irq(&tg->dl_bandwidth.dl_runtime_lock);
-- 
2.14.1

